{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\nColorization autoencoder<br>\n", "The autoencoder is trained with grayscale images as input<br>\n", "and colored images as output.<br>\n", "Colorization autoencoder can be treated like the opposite<br>\n", "of denoising autoencoder. Instead of removing noise, colorization<br>\n", "adds noise (color) to the grayscale image.<br>\n", "Grayscale Images --> Colorization --> Color Images<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from __future__ import absolute_import\n", "from __future__ import division\n", "from __future__ import print_function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from keras.layers import Dense, Input\n", "from keras.layers import Conv2D, Flatten\n", "from keras.layers import Reshape, Conv2DTranspose\n", "from keras.models import Model\n", "from keras.callbacks import ReduceLROnPlateau\n", "from keras.callbacks import ModelCheckpoint\n", "from keras.datasets import cifar10\n", "from keras.utils import plot_model\n", "from keras import backend as K"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "import os"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def rgb2gray(rgb):\n", "    \"\"\"Convert from color image (RGB) to grayscale.\n", "       Source: opencv.org\n", "       grayscale = 0.299*red + 0.587*green + 0.114*blue\n", "    Argument:\n", "        rgb (tensor): rgb image\n", "    Return:\n", "        (tensor): grayscale image\n", "    \"\"\"\n", "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["load the CIFAR10 data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["(x_train, _), (x_test, _) = cifar10.load_data()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["input image dimensions<br>\n", "we assume data format \"channels_last\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["img_rows = x_train.shape[1]\n", "img_cols = x_train.shape[2]\n", "channels = x_train.shape[3]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["create saved_images folder"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["imgs_dir = 'saved_images'\n", "save_dir = os.path.join(os.getcwd(), imgs_dir)\n", "if not os.path.isdir(save_dir):\n", "        os.makedirs(save_dir)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["display the 1st 100 input images (color and gray)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["imgs = x_test[:100]\n", "imgs = imgs.reshape((10, 10, img_rows, img_cols, channels))\n", "imgs = np.vstack([np.hstack(i) for i in imgs])\n", "plt.figure()\n", "plt.axis('off')\n", "plt.title('Test color images (Ground  Truth)')\n", "plt.imshow(imgs, interpolation='none')\n", "plt.savefig('%s/test_color.png' % imgs_dir)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["convert color train and test images to gray"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_train_gray = rgb2gray(x_train)\n", "x_test_gray = rgb2gray(x_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["display grayscale version of test images"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["imgs = x_test_gray[:100]\n", "imgs = imgs.reshape((10, 10, img_rows, img_cols))\n", "imgs = np.vstack([np.hstack(i) for i in imgs])\n", "plt.figure()\n", "plt.axis('off')\n", "plt.title('Test gray images (Input)')\n", "plt.imshow(imgs, interpolation='none', cmap='gray')\n", "plt.savefig('%s/test_gray.png' % imgs_dir)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["normalize output train and test color images"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_train = x_train.astype('float32') / 255\n", "x_test = x_test.astype('float32') / 255"]}, {"cell_type": "markdown", "metadata": {}, "source": ["normalize input train and test grayscale images"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_train_gray = x_train_gray.astype('float32') / 255\n", "x_test_gray = x_test_gray.astype('float32') / 255"]}, {"cell_type": "markdown", "metadata": {}, "source": ["reshape images to row x col x channel for CNN output/validation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, channels)\n", "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, channels)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["reshape images to row x col x channel for CNN input"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_train_gray = x_train_gray.reshape(x_train_gray.shape[0], img_rows, img_cols, 1)\n", "x_test_gray = x_test_gray.reshape(x_test_gray.shape[0], img_rows, img_cols, 1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["network parameters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["input_shape = (img_rows, img_cols, 1)\n", "batch_size = 32\n", "kernel_size = 3\n", "latent_dim = 256\n", "# encoder/decoder number of CNN layers and filters per layer\n", "layer_filters = [64, 128, 256]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["build the autoencoder model<br>\n", "first build the encoder model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["inputs = Input(shape=input_shape, name='encoder_input')\n", "x = inputs\n", "# stack of Conv2D(64)-Conv2D(128)-Conv2D(256)\n", "for filters in layer_filters:\n", "    x = Conv2D(filters=filters,\n", "               kernel_size=kernel_size,\n", "               strides=2,\n", "               activation='relu',\n", "               padding='same')(x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["shape info needed to build decoder model so we don't do hand computation<br>\n", "the input to the decoder's first Conv2DTranspose will have this shape<br>\n", "shape is (4, 4, 256) which is processed by the decoder back to (32, 32, 3)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["shape = K.int_shape(x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["generate a latent vector"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = Flatten()(x)\n", "latent = Dense(latent_dim, name='latent_vector')(x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["instantiate encoder model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["encoder = Model(inputs, latent, name='encoder')\n", "encoder.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["build the decoder model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n", "x = Dense(shape[1]*shape[2]*shape[3])(latent_inputs)\n", "x = Reshape((shape[1], shape[2], shape[3]))(x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["stack of Conv2DTranspose(256)-Conv2DTranspose(128)-Conv2DTranspose(64)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for filters in layer_filters[::-1]:\n", "    x = Conv2DTranspose(filters=filters,\n", "                        kernel_size=kernel_size,\n", "                        strides=2,\n", "                        activation='relu',\n", "                        padding='same')(x)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["outputs = Conv2DTranspose(filters=channels,\n", "                          kernel_size=kernel_size,\n", "                          activation='sigmoid',\n", "                          padding='same',\n", "                          name='decoder_output')(x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["instantiate decoder model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["decoder = Model(latent_inputs, outputs, name='decoder')\n", "decoder.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["autoencoder = encoder + decoder<br>\n", "instantiate autoencoder model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["autoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\n", "autoencoder.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["prepare model saving directory."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["save_dir = os.path.join(os.getcwd(), 'saved_models')\n", "model_name = 'colorized_ae_model.{epoch:03d}.h5'\n", "if not os.path.isdir(save_dir):\n", "        os.makedirs(save_dir)\n", "filepath = os.path.join(save_dir, model_name)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["reduce learning rate by sqrt(0.1) if the loss does not improve in 5 epochs"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n", "                               cooldown=0,\n", "                               patience=5,\n", "                               verbose=1,\n", "                               min_lr=0.5e-6)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["save weights for future use (e.g. reload parameters w/o training)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["checkpoint = ModelCheckpoint(filepath=filepath,\n", "                             monitor='val_loss',\n", "                             verbose=1,\n", "                             save_best_only=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Mean Square Error (MSE) loss function, Adam optimizer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["autoencoder.compile(loss='mse', optimizer='adam')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["called every epoch"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["callbacks = [lr_reducer, checkpoint]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["train the autoencoder"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["autoencoder.fit(x_train_gray,\n", "                x_train,\n", "                validation_data=(x_test_gray, x_test),\n", "                epochs=30,\n", "                batch_size=batch_size,\n", "                callbacks=callbacks)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["predict the autoencoder output from test data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_decoded = autoencoder.predict(x_test_gray)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["display the 1st 100 colorized images"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["imgs = x_decoded[:100]\n", "imgs = imgs.reshape((10, 10, img_rows, img_cols, channels))\n", "imgs = np.vstack([np.hstack(i) for i in imgs])\n", "plt.figure()\n", "plt.axis('off')\n", "plt.title('Colorized test images (Predicted)')\n", "plt.imshow(imgs, interpolation='none')\n", "plt.savefig('%s/colorized.png' % imgs_dir)\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}